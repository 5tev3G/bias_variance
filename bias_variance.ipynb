{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Understanding Bias and Variance in Machine Learning\n",
    "\n",
    "## Bias Explained\n",
    "\n",
    "* **Definition:** Bias refers to the tendency of a model to consistently make predictions that are wrong in a particular direction. It measures how far, on average, the model's predictions deviate from the correct values.\n",
    "* **Underfitting (High Bias, Low Variance):**\n",
    "    * **Training Data:** A model that's too simple can't capture the underlying patterns in the training data. This leads to underfitting.\n",
    "    * **Test Data:** Underfitted models perform poorly on both the training and test data because they fail to generalize to new data.\n",
    "    * **Example:** Using a simple linear model to predict house prices based on square footage might miss the complex relationship between these factors. This high bias leads to consistent underestimation or overestimation of price, resulting in poor performance on both datasets.\n",
    "\n",
    "## Variance Explained\n",
    "\n",
    "* **Definition:** Variance refers to the model's sensitivity to the specific training data. Low variance models consistently make similar predictions irrespective of noise or variations in the data. \n",
    "* **Overfitting (Low Bias, High Variance):**\n",
    "    * **Training Data:** Overfitted models fit the training data too closely, capturing noise and random fluctuations. This memorizes the data rather than learning the underlying patterns.\n",
    "    * **Test Data:** Overfitted models perform poorly on the test data because they haven't learned to generalize.\n",
    "    * **Example:** A very complex model might perfectly fit the training data on house prices, but perform poorly on new houses. This is because it has learned the noise in the data, leading to high variance.\n",
    "\n",
    "## The Bias-Variance Trade-off\n",
    "\n",
    "The goal is to find a sweet spot between bias and variance:\n",
    "\n",
    "* A model with both high bias and high variance will perform poorly.\n",
    "* Ideally, we want a model with low bias and low variance.\n",
    "\n",
    "**Complexity and the Trade-off:**\n",
    "\n",
    "* As the model complexity increases, bias typically decreases (it can better fit the data), but variance increases (it becomes more sensitive to noise).\n",
    "* Conversely, decreasing complexity increases bias (the model misses patterns) and decreases variance (less sensitive to noise).\n",
    "\n",
    "## Key Points to Remember\n",
    "\n",
    "* **Model Complexity:** Simpler models are more prone to underfitting, while complex models are more likely to overfit.\n",
    "* **Data Quality and Quantity:** High-quality and sufficient training data can help reduce both bias and variance.\n",
    "* **Regularization Techniques:** Techniques like L1 and L2 regularization can help reduce overfitting by penalizing complex models.\n",
    "* **Cross-Validation:** This technique helps assess the model's performance on unseen data and identify potential overfitting or underfitting.\n",
    "* **Feature Selection:** Reducing the number of features can prevent overfitting.\n",
    "* **More Data:** Increased training data can improve generalization and reduce both bias and variance.\n"
   ],
   "id": "f05c3b3ccceceda7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T02:39:34.371257Z",
     "start_time": "2024-11-19T02:39:34.368868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import  PolynomialFeatures\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual"
   ],
   "id": "30cf9f00ebd3ed32",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T02:39:35.376085Z",
     "start_time": "2024-11-19T02:39:35.260959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate some noisy data\n",
    "np.random.seed(42)\n",
    "x = np.linspace(0, 10, 30)\n",
    "y = 2 * x + 5 + np.random.randn(30) * 2\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x.reshape(-1, 1), y, test_size=0.2)\n",
    "\n",
    "# Create polynomial models of different degrees\n",
    "def polynomial_regression(degree):\n",
    "    return Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree)),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "\n",
    "def plot_regression(model_type, show_train=True, show_test=True):\n",
    "    if model_type == 'Underfitting':\n",
    "        degree = 1\n",
    "    elif model_type == 'Balanced':\n",
    "        degree = 5\n",
    "    else:\n",
    "        degree = 15\n",
    "\n",
    "    model = polynomial_regression(degree)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(x[:, np.newaxis])\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(x, y, label='Data')\n",
    "    plt.plot(x, y_pred, label=f'{model_type} Model')\n",
    "\n",
    "    if show_train:\n",
    "        plt.scatter(X_train, y_train, color='red', label='Training Data')\n",
    "        #plt.scatter(X_test, y_test, color='gold') # test data\n",
    "    if show_test:\n",
    "        plt.scatter(X_test, y_test, color='Purple', label='Testing Data')\n",
    "\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.title(f'{model_type} Model')\n",
    "    plt.show()\n",
    "\n",
    "# Interactive plot\n",
    "interact(plot_regression, model_type=['Underfitting', 'Balanced', 'Overfitting'], show_train=True, show_test=True)"
   ],
   "id": "65dab6b68ad51fa9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interactive(children=(Dropdown(description='model_type', options=('Underfitting', 'Balanced', 'Overfitting'), â€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55ea92b037ae473ea842a2ca1af920b6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_regression(model_type, show_train=True, show_test=True)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "510ac6a64d1d91a2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
