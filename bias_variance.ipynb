{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f05c3b3ccceceda7",
   "metadata": {},
   "source": [
    "# Understanding Bias and Variance in Machine Learning\n",
    "\n",
    "## Bias Explained\n",
    "\n",
    "* **Definition:** Bias refers to the tendency of a model to consistently make predictions that are wrong in a particular direction. It measures how far, on average, the model's predictions deviate from the correct values.\n",
    "* **Underfitting (High Bias, Low Variance):**\n",
    "    * **Training Data:** A model that's too simple can't capture the underlying patterns in the training data. This leads to underfitting.\n",
    "    * **Test Data:** Underfitted models perform poorly on both the training and test data because they fail to generalize to new data.\n",
    "    * **Example:** Using a simple linear model to predict house prices based on square footage might miss the complex relationship between these factors. This high bias leads to consistent underestimation or overestimation of price, resulting in poor performance on both datasets.\n",
    "\n",
    "    * ![Underfitting](underfitting.png)\n",
    "\n",
    "## Variance Explained\n",
    "\n",
    "* **Definition:** Variance refers to the model's sensitivity to the specific training data. Low variance models consistently make similar predictions irrespective of noise or variations in the data. \n",
    "* **Overfitting (Low Bias, High Variance):**\n",
    "    * **Training Data:** Overfitted models fit the training data too closely, capturing noise and random fluctuations. This memorizes the data rather than learning the underlying patterns.\n",
    "    * **Test Data:** Overfitted models perform poorly on the test data because they haven't learned to generalize.\n",
    "    * **Example:** A very complex model might perfectly fit the training data on house prices, but perform poorly on new houses. This is because it has learned the noise in the data, leading to high variance.\n",
    "\n",
    "     * ![Overfitting](overfitting.png)\n",
    "\n",
    "## The Bias-Variance Trade-off\n",
    "\n",
    "The goal is to find a sweet spot between bias and variance:\n",
    "\n",
    "* A model with both high bias and high variance will perform poorly.\n",
    "* Ideally, we want a model with low bias and low variance.\n",
    "\n",
    "**Complexity and the Trade-off:**\n",
    "\n",
    "* As the model complexity increases, bias typically decreases (it can better fit the data), but variance increases (it becomes more sensitive to noise).\n",
    "* Conversely, decreasing complexity increases bias (the model misses patterns) and decreases variance (less sensitive to noise).\n",
    "\n",
    "## Key Points to Remember\n",
    "\n",
    "* **Model Complexity:** Simpler models are more prone to underfitting, while complex models are more likely to overfit.\n",
    "* **Data Quality and Quantity:** High-quality and sufficient training data can help reduce both bias and variance.\n",
    "* **Regularization Techniques:** Techniques like L1 and L2 regularization can help reduce overfitting by penalizing complex models.\n",
    "* **Cross-Validation:** This technique helps assess the model's performance on unseen data and identify potential overfitting or underfitting.\n",
    "* **Feature Selection:** Reducing the number of features can prevent overfitting.\n",
    "* **More Data:** Increased training data can improve generalization and reduce both bias and variance.\n",
    "\n",
    "## Demo\n",
    "\n",
    "Imagine you're trying to fit a curve to some data points. These data points represent a relationship between an input value (x) and an output value (y). In this case, the relationship is a straight line with some added noise (randomness).\n",
    "\n",
    "The demo creates different models to fit this data. These models are like different curves you can try to draw through the data points. The \"degree\" of the model determines how complex the curve can be.\n",
    "\n",
    "Underfitting: Imagine trying to fit a straight line (degree 1) to data that actually curves a bit. The model won't capture all the details of the data, and its predictions might not be very accurate, especially for unseen data points.  \n",
    "\n",
    "Balanced Model: This is a model with a medium complexity. Think of it as a curve that bends a bit more than a straight line, but not so much that it wiggles around the data points. This model captures the main trend of the data and can make good predictions for both training data (data it saw during training) and unseen data (data it hasn't seen before).  \n",
    "\n",
    "Overfitting: This happens when the model is too complex. Imagine trying to fit a very squiggly curve (degree 15) to the data. This model might perfectly capture all the random noise in the training data, but it won't generalize well to unseen data. The squiggly curve might not follow the actual trend of the data, and its predictions for unseen data points will probably be inaccurate.  \n",
    "\n",
    "Use the demo to plot the data and see the predictions of these different models, you can see how underfitting and overfitting affect the model's ability to learn the true relationship between the input and output values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30cf9f00ebd3ed32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T16:44:14.585349Z",
     "start_time": "2024-11-19T16:43:53.951757Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import  PolynomialFeatures\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65dab6b68ad51fa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T16:44:14.974869Z",
     "start_time": "2024-11-19T16:44:14.586388Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ddd171c93544d1a020e8e1285077b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='model_type', options=('Underfitting', 'Balanced', 'Overfitting'), â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_regression(model_type, show_train=True, show_test=True)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate some noisy data\n",
    "np.random.seed(42)\n",
    "x = np.linspace(0, 10, 30)\n",
    "y = 2 * x + 5 + np.random.randn(30) * 2\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x.reshape(-1, 1), y, test_size=0.2)\n",
    "\n",
    "# Create polynomial models of different degrees\n",
    "def polynomial_regression(degree):\n",
    "    return Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=degree)),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "\n",
    "def plot_regression(model_type, show_train=True, show_test=True):\n",
    "    if model_type == 'Underfitting':\n",
    "        degree = 1\n",
    "    elif model_type == 'Balanced':\n",
    "        degree = 5\n",
    "    else:\n",
    "        degree = 15\n",
    "\n",
    "    model = polynomial_regression(degree)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(x[:, np.newaxis])\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(x, y, label='Data')\n",
    "    plt.plot(x, y_pred, label=f'{model_type} Model')\n",
    "\n",
    "    if show_train:\n",
    "        plt.scatter(X_train, y_train, color='red', label='Training Data')\n",
    "        #plt.scatter(X_test, y_test, color='gold') # test data\n",
    "    if show_test:\n",
    "        plt.scatter(X_test, y_test, color='Purple', label='Testing Data')\n",
    "\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.title(f'{model_type} Model')\n",
    "    plt.show()\n",
    "\n",
    "# Interactive plot\n",
    "interact(plot_regression, model_type=['Underfitting', 'Balanced', 'Overfitting'], show_train=True, show_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ac6a64d1d91a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
